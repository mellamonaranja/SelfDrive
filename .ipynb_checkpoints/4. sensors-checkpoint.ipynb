{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "472abcde-99e2-4bd2-89d4-63453d0b8819",
   "metadata": {},
   "source": [
    "#### How to stream the data from a single camera to the screen with 6 different types of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be64a3c-adc0-41af-bda2-9552373122ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import carla\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3751a63f-76b5-4ed2-befa-051ddeb72f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client=carla.Client('localhost',2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc613f3a-980c-4948-93c6-05e5d8312214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world=client.get_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee8feb6-8625-45a6-afde-c76801a98c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bp_lib=world.get_blueprint_library()\n",
    "spawn_points=world.get_map().get_spawn_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e80e26-b405-4ecf-90cc-252e11ed355d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(len(spawn_points))\n",
    "# print(spawn_points[79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "379406c1-8735-490c-a8d8-cd8d5501db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_bp=bp_lib.find('vehicle.lincoln.mkz_2020')\n",
    "vehicle=world.try_spawn_actor(vehicle_bp, spawn_points[79])\n",
    "# vehicle=world.try_spawn_actor(vehicle_bp, random.choice(spawn_points))\n",
    "\n",
    "spectator=world.get_spectator()\n",
    "transform = carla.Transform(vehicle.get_transform().transform(carla.Location(x=-4,z=2.5)),vehicle.get_transform().rotation) \n",
    "spectator.set_transform(transform)\n",
    "\n",
    "#spawn_actor(self, blueprint, transform, attach_to=None, attachment=Rigid) : \n",
    "#shows some sensors being attached to a car when spawned\n",
    "\n",
    "for i in range(50):\n",
    "    vehicle_bp=random.choice(bp_lib.filter('vehicle'))\n",
    "    npc=world.try_spawn_actor(vehicle_bp, random.choice(spawn_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec717c3-1fe1-497f-96ee-65d69c83a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set traffic in motion\n",
    "\n",
    "for v in world.get_actors().filter('*vehicle*'):\n",
    "    v.set_autopilot(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558608f7-1ace-4972-8d39-f3370021341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial camera translation\n",
    "camera_init_trans=carla.Transform(carla.Location(z=2))\n",
    "\n",
    "# Add one of each type of cameras\n",
    "#spawn_actor(self, blueprint, transform, attach_to=None, attachment=Rigid) : shows some sensors being attached to a car when spawned\n",
    "\n",
    "camera_bp=bp_lib.find('sensor.camera.rgb')\n",
    "camera=world.spawn_actor(camera_bp, camera_init_trans, attach_to=vehicle) \n",
    " \n",
    "sem_camera_bp=bp_lib.find('sensor.camera.semantic_segmentation')\n",
    "sem_camera=world.spawn_actor(sem_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "inst_camera_bp=bp_lib.find('sensor.camera.instance_segmentation')\n",
    "inst_camera=world.spawn_actor(inst_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "depth_camera_bp=bp_lib.find('sensor.camera.depth')\n",
    "depth_camera=world.spawn_actor(depth_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "dvs_camera_bp=bp_lib.find('sensor.camera.dvs')\n",
    "dvs_camera=world.spawn_actor(dvs_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "opt_camera_bp=bp_lib.find('sensor.camera.optical_flow')\n",
    "opt_camera=world.spawn_actor(opt_camera_bp, camera_init_trans, attach_to=vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6e6d57a-c0e2-4c61-9a2e-1c40890403bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\workspace\\\\carla\\\\carla\\\\exercise'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c22ce131-8d50-43f9-8e94-6b037f459628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define respective callbacks\n",
    "\n",
    "#(overall rows, inside rows, columns)\n",
    "def rgb_callback(image, data_dict):\n",
    "    data_dict['rgb_image']=np.reshape(np.copy(image.raw_data), (image.height, image.width, 4)) \n",
    "\n",
    "# CityScapesPalette\n",
    "# semantic camera gives a different integer for each different class of object\n",
    "# the integers are between 0 and 22\n",
    "# in order to visualize it,use CityScapesPalette\n",
    "\n",
    "def sem_callback(image, data_dict):\n",
    "    image.convert(carla.ColorConverter.CityScapesPalette)\n",
    "    data_dict['sem_image']=np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "\n",
    "def inst_callback(image, data_dict):\n",
    "    data_dict['inst_image']=np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "\n",
    "# LogarithmicDepth\n",
    "# Converts the image to a depth map using a logarithmic scale, leading to better precision for small distances at the expense of losing it when further away\n",
    "def depth_callback(image, data_dict):\n",
    "    image.convert(carla.ColorConverter.LogarithmicDepth)\n",
    "    data_dict['depth_image']=np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "\n",
    "#get_color_coded_flow(self) :\n",
    "#Visualization helper. Converts the optical flow image to an RGB image.\n",
    "#Return: carla.Image\n",
    "    \n",
    "# extract the data from the camera and then reshape\n",
    "\n",
    "def opt_callback(data, data_dict):\n",
    "    image=data.get_color_coded_flow()\n",
    "    img=np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    img[:,:,3]=255 #hm, why???.0  \n",
    "    data_dict['opt_image']=img\n",
    "    \n",
    "def dvs_callback(data, data_dict):\n",
    "    dvs_events=np.frombuffer(data.raw_data, dtype=np.dtype([('x', np.uint16), ('y', np.uint16), ('t', np.int64), ('pol', bool)])) #why uint16, 64???\n",
    "    data_dict['dvs_image']=np.zeros((data.height, data.width, 4), dtype=np.uint8)\n",
    "    dvs_img=np.zeros((data.height, data.width, 3), dtype=np.uint8)\n",
    "    dvs_img[dvs_events[:]['y'],dvs_events[:]['x'],dvs_events[:]['pol']*2]=255\n",
    "    data_dict['dvs_image'][:,:,0:3]=dvs_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9966f9a-ddc1-43d8-8a7b-33c4fd61e3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initialize parameters and data\n",
    "\n",
    "image_w=camera_bp.get_attribute('image_size_x').as_int()\n",
    "image_h=camera_bp.get_attribute('image_size_y').as_int()\n",
    "\n",
    "sensor_data={'rgb_image':np.zeros((image_h, image_w, 4)),\n",
    "            'sem_image':np.zeros((image_h, image_w, 4)),\n",
    "            'depth_image':np.zeros((image_h, image_w, 4)),\n",
    "            'dvs_image':np.zeros((image_h, image_w, 4)),\n",
    "             'opt_image': np.zeros((image_h, image_w, 4)),\n",
    "             'inst_image':np.zeros((image_h, image_w, 4))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9c74f-a2f2-49b9-8b7b-1c920b2aa5f9",
   "metadata": {},
   "source": [
    "# computer vision\n",
    "    the way of teaching intelligence to machine and making them see things like humans. What allows computers to see and process visual data like humans, analyzing images to produce useful information\n",
    "\n",
    "# OpenCV\n",
    "    vision library of programming functions mainly aimed at real time computer vision. it can be showed a digital image seen by a computer in matrix\n",
    "    \n",
    "# namedWindow() :\n",
    "    is used to create a window with a suitable name and size to display images and videos on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97f077a9-1f7d-43fa-8b2b-ce7fa96a5852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OpenCV named window for display\n",
    "\n",
    "cv2.namedWindow('All cameras', cv2.WINDOW_AUTOSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3fcd06c-ba70-4125-92c8-1e2be4ec6813",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 2400, 4)\n"
     ]
    }
   ],
   "source": [
    "# tile all data in one array\n",
    "\n",
    "top_row = np.concatenate((sensor_data['rgb_image'], sensor_data['sem_image'], sensor_data['inst_image']), axis=1)\n",
    "lower_row = np.concatenate((sensor_data['depth_image'], sensor_data['dvs_image'], sensor_data['opt_image']), axis=1)\n",
    "tiled = np.concatenate((top_row, lower_row), axis=0)\n",
    "\n",
    "# print(tiled)\n",
    "print(tiled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebb977e9-2168-4ab4-9ea0-e9f9c1446f92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using cv2.imshow() to display the image\n",
    "cv2.imshow('All cameras', tiled)\n",
    "\n",
    "# Waiting 1ms for user to press any key\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be740bb5-c564-47bc-9b30-77f6b5720ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set sensors recording\n",
    "\n",
    "camera.listen(lambda image:rgb_callback(image, sensor_data))\n",
    "sem_camera.listen(lambda image:sem_callback(image, sensor_data))\n",
    "inst_camera.listen(lambda image:inst_callback(image, sensor_data))\n",
    "depth_camera.listen(lambda image:depth_callback(image, sensor_data))\n",
    "dvs_camera.listen(lambda image:dvs_callback(image, sensor_data))\n",
    "opt_camera.listen(lambda image:opt_callback(image, sensor_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13dc8b57-e9e2-4008-973a-77847d791097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bccc84f8-c61b-4eb1-9b8f-3ef3776bdd60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "sem_camera.stop()\n",
    "inst_camera.stop()\n",
    "depth_camera.stop()\n",
    "dvs_camera.stop()\n",
    "opt_camera.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246a731-58c7-4d48-b98b-4b4dd45e234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Tile camera images into one array\n",
    "    top_row=np.concatenate((sensor_data['rgb_image'],sensor_data['sem_image'],sensor_data['inst_image']), axis=1)\n",
    "    lower_row=np.concatenate((sensor_data['depth_image'], sensor_data['dvs_image'], sensor_data['opt_image']),axis=1)\n",
    "    tiled=np.concatenate((top_row, lower_row), axis=0)\n",
    "    \n",
    "    print(top_row.shape)\n",
    "    print(top_row[0])\n",
    "    \n",
    "    print(lower_row.shape)\n",
    "    print(lower_row[0])\n",
    "    \n",
    "    print(tiled.shape)\n",
    "    print(tiled[0])\n",
    "    \n",
    "    cv2.imshow('All cameras', tiled)\n",
    "    \n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998622c-8728-487c-9170-411a4c91c46c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c0368-5d76-4fe4-b282-df4a4f231aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e0894de-4483-4f89-837a-042e9688c8be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a = np.array([[1, 2]])\n",
    "# b = np.array([[5, 6]])\n",
    "# row_wise=np.concatenate((a, b), axis=0)\n",
    "# column_wise=np.concatenate((a, b), axis=1)\n",
    "\n",
    "# print(row_wise)\n",
    "# print(column_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15366a-fb4c-4ceb-ad50-ec2df79faba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd730533-071e-4be9-a2e3-3575c1005c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.arange(3, dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a69e19-a01c-4422-8fcb-08be6dc791c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = b'Welcome User'\n",
    "# np.frombuffer(st, dtype='S1', count=5, offset=6)\n",
    "# result=np.array([b't', b'e', b'r', b'y', b'e'], dtype='|S1')\n",
    "# st1 = b'Welcome User'\n",
    "# np.frombuffer(st1, dtype='S1', count=5, offset=6)\n",
    "# result1=np.array([b'rr', b'df', b'hd', b'f', b'df'], dtype='|S1')\n",
    "# st2 = b'Welcome User'\n",
    "# np.frombuffer(st2, dtype='S1', count=5, offset=6)\n",
    "# result2=np.array([b'tee', b'eeee', b'errr', b'eey', b'ehd'], dtype='|S1')\n",
    "# st3 = b'Welcome User'\n",
    "# np.frombuffer(st3, dtype='S1', count=5, offset=6)\n",
    "# result3=np.array([b'tff', b'eee', b'rww', b'wwy', b'eyg'], dtype='|S4')\n",
    "\n",
    "\n",
    "# print(result)\n",
    "# print(result1)\n",
    "# print(result2)\n",
    "# print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "657908ac-1187-4400-b0bd-fbfa1a335fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.zeros((2,2,4))\n",
    "a\n",
    "# np.reshape(np.copy(a), 2,2) \n",
    "\n",
    "# b=a.T\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bc803-477c-4019-aac3-0864ce2759e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
